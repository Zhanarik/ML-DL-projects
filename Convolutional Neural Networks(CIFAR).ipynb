{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "5PRnEnK_X4IK"
   },
   "source": [
    "# Deep learning for computer vision\n",
    "\n",
    "\n",
    "This notebook will teach you to build and train convolutional networks for image recognition. Brace yourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1cxU7sdX4IT"
   },
   "source": [
    "# CIFAR dataset\n",
    "This week, we shall focus on the image recognition problem on cifar10 dataset\n",
    "* 60k images of shape 3x32x32\n",
    "* 10 different classes: planes, dogs, cats, trucks, etc.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_DL/blob/fall22/week03_convnets/cifar10.jpg?raw=1\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoPAbpSGX4IW",
    "outputId": "000f19f1-a7c2-4446-8620-ebb894eec6a9"
   },
   "outputs": [],
   "source": [
    "# when running in colab, un-comment this\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall19/week03_convnets/cifar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUZ83df4X4IZ",
    "outputId": "be5345dc-976f-4e3c-b567-813c1e9ae3ef"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cifar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f630bc5559c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcifar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_cifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cifar'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cifar import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
    "\n",
    "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "37mWViJoX4Ib",
    "outputId": "a7a1c6b2-ad23-4825-a55c-cc1f1214a796"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-428625f2906b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMEAAADDCAYAAAA7inWXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAACf1JREFUeJzt3X+oX3Udx/HnS02lZTqbgajXTZqtWcHml5KEtFw4F8zCig0kteXNMgmUIFmY2B9lQYJk2SgxheaP/RE3WpjpRIiueoc6daHOaTWTNucUQlwq7/4459Z3X+/1nrvv53zvne/XAy4733M+55w3Z3vd8z3f8915KyIwy+ygmS7AbKY5BJaeQ2DpOQSWnkNg6TkElt6UIZB0k6Sdkh6fZLkkXS9pm6QtkpaWL9OsPU3OBDcDy99m+TnAwvpnGPh5/2WZDc6UIYiI+4GX3mbIucAtURkFjpJ0bKkCzdpW4prgOOAfXa931PPMDgiHDHJnkoap3jIxZ86cUxctWjTI3ds70ObNm1+MiGP62UaJEDwPnND1+vh63ltExDpgHUCn04mxsbECu7fMJP2t322UeDs0Any5/pToNOCViHihwHbNBmLKM4Gk9cCZwDxJO4DvAe8CiIgbgY3ACmAb8CpwUVvFmrVhyhBExOoplgdwabGKzAbMd4wtPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCy9RiGQtFzSk3UPgu9MsHxI0iZJD9c9ClaUL9WsHU2adBwM3EDVh2AxsFrS4p5h3wXuiIglwCrgZ6ULNWtLkzPBx4BtEbE9Iv4D3EbVk6BbAO+tp48E/lmuRLN2NXkq9UT9Bz7eM+Zq4I+SLgPmAMuKVGc2AKUujFcDN0fE8VQP571V0lu2LWlY0piksV27dhXatVl/moSgSf+BNcAdABHxF+BwYF7vhiJiXUR0IqJzzDF99VUwK6ZJCB4CFkpaIOlQqgvfkZ4xfwfOApD0IaoQ+Fe9HRCaNO57A/gmcBfwV6pPgZ6QdI2klfWwK4CLJT0KrAcurB/ZbjbrNWrXFBEbqZpxdM+7qmt6K3B62dLMBsN3jC09h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILL0i/QnqMV+StFXSE5J+U7ZMs/ZM+fCtrv4En6F6IvVDkkbqB26Nj1kIXAmcHhF7JL2/rYLNSivVn+Bi4IaI2AMQETvLlmnWniYhmKg/wXE9Y04GTpb0Z0mjkpaXKtCsbY2eRdpwOwuBM6ke3X6/pI9ExMvdgyQNA8MAQ0NDhXZt1p9S/Ql2ACMR8XpEPAs8RRWKfbg/gc1GpfoT/JbqLICkeVRvj7YXrNOsNaX6E9wF7Ja0FdgEfDsidrdVtFlJmqleGp1OJ8bGxmZk3/bOIWlzRHT62YbvGFt6DoGl5xBYeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHrF+hPU486TFJL6eg6M2SBNGYKu/gTnAIuB1ZIWTzDuCOBbwAOlizRrU6n+BADfB64FXitYn1nrivQnkLQUOCEifl+wNrOB6PvCWNJBwE+AKxqMHZY0Jmls165d/e7arIgS/QmOAD4M3CfpOeA0YGSii2P3J7DZqO/+BBHxSkTMi4j5ETEfGAVWRoQfOW0HhFL9CcwOWI16lkXERmBjz7yrJhl7Zv9lmQ2O7xhbeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpecQWHoOgaXnEFh6RfoTSLpc0lZJWyTdI+nE8qWataNUf4KHgU5EfBTYAPyodKFmbSnSnyAiNkXEq/XLUaqH9podEIr0J+ixBvhDP0WZDVKjZ5E2Jel8oAOcMcnyYWAYYGhoqOSuzfZbif4EAEhaBqyleiz73ok25P4ENhv13Z8AQNIS4BdUAdhZvkyz9pTqT/Bj4D3AnZIekTQyyebMZp0i/QkiYlnhuswGxneMLT2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsPYfA0nMILD2HwNJzCCw9h8DScwgsvVL9CQ6TdHu9/AFJ80sXataWUv0J1gB7IuIDwHXAtaULNWtLkf4E9etf19MbgLMkqVyZZu0p1Z/gf2PqZ5e+AryvRIFmbSvan2Aq3f0JgL2SHh/k/qcwD3hxpovoMdtqmm31AHyw3w00CUGT/gTjY3ZIOgQ4Etjdu6GIWAesA5A0FhGd/Sm6DbOtHph9Nc22eqCqqd9tFOlPUL++oJ7+AnBvRES/xZkNwpRngoh4Q9J4f4KDgZvG+xMAYxExAvwKuFXSNuAlqqCYHRBK9Sd4DfjiNPe9bprj2zbb6oHZV9NsqwcK1CS/a7Hs/LUJS6+VEPTzNQtJV9bzn5R09oDquVzSVklbJN0j6cSuZW/WfdiK9WJrUM+FknZ17ferXcsukPR0/XNB77ot1nRdVz1PSXq5a1kbx+gmSTsn+xhdlevrerdIWtq1bHrHKCKK/lBdPD8DnAQcCjwKLO4Z8w3gxnp6FXB7Pb24Hn8YsKDezsEDqOdTwLvr6a+P11O//vcMHJ8LgZ9OsO7RwPb6z7n19NxB1NQz/jKqD0haOUb1Nj8JLAUen2T5Cqqm8QJOAx7Y32PUxpmgn69ZnAvcFhF7I+JZYFu9vVbriYhNEfFq/XKU6l5IW5ocn8mcDdwdES9FxB7gbmD5DNS0GlhfYL+Tioj7qT5pnMy5wC1RGQWOknQs+3GM2ghBP1+zaLJuG/V0W0P1G2bc4ZLGJI1K+lyftUynnvPq0/wGSeM3K9s4PtPabv1WcQFwb9fs0seoiclqnvYxGujXJmY7SecDHeCMrtknRsTzkk4C7pX0WEQ803IpvwPWR8ReSV+jOmt+uuV9NrUK2BARb3bNm4ljVEwbZ4LpfM2Cnq9ZNFm3jXqQtAxYC6yMiL3j8yPi+frP7cB9wJK264mI3V01/BI4tem6bdXUZRU9b4VaOEZNTFbz9I9RCxc0h1BdjCzg/xdZp/SMuZR9L4zvqKdPYd8L4+30f2HcpJ4lVBeGC3vmzwUOq6fnAU/zNheMBes5tmv688Bo10Xfs3Vdc+vpowfxd1aPWwQ8R31/qa1j1LXt+Ux+YfxZ9r0wfnB/j1HxENSFrACeqv9hra3nXUP1WxbgcOBOqgvfB4GTutZdW6/3JHDOgOr5E/Av4JH6Z6Se/wngsfofxWPAmgHV8wPgiXq/m4BFXet+pT5u24CLBvV3Vr++Gvhhz3ptHaP1wAvA61Tv69cAlwCX1MtF9Z+9nqn329nfY+Q7xpae7xhbeg6BpecQWHoOgaXnEFh6DoGl5xBYeg6BpfdfQiDFJFOWA2oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=[12,10])\n",
    "for i in range(12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "    plt.imshow(np.transpose(X_train[i],[1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLxkLploX4Ie"
   },
   "source": [
    "# Building a network\n",
    "\n",
    "Simple neural networks with layers applied on top of one another can be implemented as `torch.nn.Sequential` - just add a list of pre-built modules and let it train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "id": "9-4LgynPX4Ig"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QElX1t2dPR7",
    "outputId": "c85b7090-059e-4fb6-ee6b-01e7c9953abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  6 18:21:03 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0    31W /  70W |   1606MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Oi7kUnvCdG0M"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 3).to('cuda')\n",
    "\n",
    "y = torch.randn(3, 2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oCO3FPRdKO2",
    "outputId": "057f3bbc-162b-4783-db1f-92317624e676"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3276,  0.8346],\n",
       "        [ 0.3788, -1.1072],\n",
       "        [ 1.8094, -1.2321]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTiNmF3GX4Ii"
   },
   "source": [
    "Let's start with a dense network for our baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "id": "f-xu9f94X4Ik"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=(5, 5)), # [32, 28, 28]\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d((3, 3)),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(32, 64, kernel_size=(5, 5)), # [64, 5, 5]\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    Flatten(), # [b, c, h, w] -> [b, flat_vector]\n",
    "    nn.Linear(64 * 5 * 5, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4BtGG6_X4Iy"
   },
   "source": [
    "As in our basic tutorial, we train our model with negative log-likelihood aka crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "id": "L6kLXDaaX4I4"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = torch.as_tensor(X_batch, dtype=torch.float32, device=device)\n",
    "    y_batch = torch.as_tensor(y_batch, dtype=torch.int64, device=device)\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SXMP4cTFX4JH",
    "outputId": "88aeb774-dd54-4abe-848a-c402518f3704"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4230, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "compute_loss(X_train[:5], y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBPc67J9X4JJ"
   },
   "source": [
    "### Training on minibatches\n",
    "* We got 40k images, that's way too many for a full-batch SGD. Let's train on minibatches instead\n",
    "* Below is a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "id": "-M-jebF7X4JK"
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "id": "O1dDHSA0X4JK"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DpEJHhwNX4JL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "\n",
    "batch_size = 50  # number of samples processed in one SGD iteration\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        logits = model(torch.as_tensor(\n",
    "            X_batch, dtype=torch.float32, device=device))\n",
    "        y_pred = logits.max(1)[1].cpu().data.numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIAmy9HWX4JN"
   },
   "source": [
    "Don't wait for full 100 epochs. You can interrupt training after 5-20 epochs once validation accuracy stops going up.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JgF4OappX4JO",
    "outputId": "d5faedde-6bd3-4db5-995a-88581a3daa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t69.58 %\n",
      "Achievement unlocked: 70lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
    "    y_pred = logits.max(1)[1].cpu().data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSHPS37CX4JQ"
   },
   "source": [
    "## Task I: small convolution net\n",
    "### First step\n",
    "\n",
    "Let's create a mini-convolutional network with roughly such architecture:\n",
    "* Input layer\n",
    "* 3x3 convolution with 10 filters and _ReLU_ activation\n",
    "* 2x2 pooling (or set previous convolution stride to 3)\n",
    "* Flatten\n",
    "* Dense layer with 100 neurons and _ReLU_ activation\n",
    "* 10% dropout\n",
    "* Output dense layer.\n",
    "\n",
    "\n",
    "__Convolutional layers__ in torch are just like all other layers, but with a specific set of parameters:\n",
    "\n",
    "__`...`__\n",
    "\n",
    "__`model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)) # convolution`__\n",
    "\n",
    "__`model.add_module('pool1', nn.MaxPool2d(2)) # max pooling 2x2`__\n",
    "\n",
    "__`...`__\n",
    "\n",
    "\n",
    "Once you're done (and compute_loss no longer raises errors), train it with __Adam__ optimizer with default params (feel free to modify the code above).\n",
    "\n",
    "If everything is right, you should get at least __50%__ validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieNL4rYqX4JS"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "__Hint:__ If you don't want to compute shapes by hand, just plug in any shape (e.g. 1 unit) and run compute_loss. You will see something like this:\n",
    "\n",
    "__`RuntimeError: size mismatch, m1: [5 x 1960], m2: [1 x 64] at /some/long/path/to/torch/operation`__\n",
    "\n",
    "See the __1960__ there? That's your actual input shape.\n",
    "\n",
    "## Task 2: adding normalization\n",
    "\n",
    "* Add batch norm (with default params) between convolution and ReLU\n",
    "  * nn.BatchNorm*d (1d for dense, 2d for conv)\n",
    "  * usually better to put them after linear/conv but before nonlinearity\n",
    "* Re-train the network with the same optimizer, it should get at least 60% validation accuracy at peak.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URQ5Gy49X4JT"
   },
   "source": [
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "## Task 3: Data Augmentation\n",
    "\n",
    "There's a powerful torch tool for image preprocessing useful to do data preprocessing and augmentation.\n",
    "\n",
    "Here's how it works: we define a pipeline that\n",
    "* makes random crops of data (augmentation)\n",
    "* randomly flips image horizontally (augmentation)\n",
    "* then normalizes it (preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "id": "jtsJrplYX4JV"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "id": "zr_mrLgsX4JX"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "train_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=32,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xb1WIqccX4JY"
   },
   "outputs": [],
   "source": [
    "\n",
    "for (x_batch, y_batch) in train_batch_gen:\n",
    "    \n",
    "    print('X:', type(x_batch), x_batch.shape)\n",
    "    print('y:', type(y_batch), y_batch.shape)\n",
    "    \n",
    "    for i, img in enumerate(x_batch.numpy()[:8]):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(img.transpose([1,2,0]) * stds + means )\n",
    "        \n",
    "    \n",
    "    # raise NotImplementedError(\"Plese use this code in your training loop\")\n",
    "    # TODO use this in your training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOmVGeUzX4JZ"
   },
   "source": [
    "When testing, we don't need random crops, just normalize with same statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8p6zg0SLX4Ja"
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "# test_loader = <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg4PYFG2X4Ja"
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsnUw4FbX4Jc"
   },
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or with weight_decay parameter of a optimizer ([for example SGD's doc](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x_AVkrBk2tN"
   },
   "source": [
    "# Report:\n",
    "\n",
    "**The best result (73.33%) I achieved by the following way:**\n",
    "\n",
    "I used baseline-model from seminar, *added dropout layer* (with probability 10%) before second convolution layer and *added early stopping*: my model stops training when validation accuracy score doesn't increase for 10 epochs.\n",
    "\n",
    "Baseline model from seminar with my early stopping method gets about 69.5% accuracy score on test\n",
    "\n",
    "Also in my experiments I changed optimizers (I tried Adam optimizer with different learning rates), combined some Dropout layers with different probabilities and used some other variants for improving model, but these approaches just decrease accuracy results on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "rGktnEe6XPzD"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=(5, 5)), # [32, 28, 28]\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d((3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1), #added dropout with prob 10%\n",
    "    nn.Conv2d(32, 64, kernel_size=(5, 5)), # [64, 5, 5]\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    Flatten(), # [b, c, h, w] -> [b, flat_vector]\n",
    "    nn.Linear(64 * 5 * 5, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "qSb-TCa5X4Jf"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RBlwiYz0aJi",
    "outputId": "362e621a-e33b-447f-9fbf-48ae4a7edd73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 2.582s\n",
      "  training loss (in-iteration): \t1.630138\n",
      "  validation accuracy: \t\t\t39.92 %\n",
      "Epoch 2 of 100 took 2.540s\n",
      "  training loss (in-iteration): \t1.277556\n",
      "  validation accuracy: \t\t\t36.60 %\n",
      "Epoch 3 of 100 took 2.534s\n",
      "  training loss (in-iteration): \t1.127789\n",
      "  validation accuracy: \t\t\t59.98 %\n",
      "Epoch 4 of 100 took 2.509s\n",
      "  training loss (in-iteration): \t1.030067\n",
      "  validation accuracy: \t\t\t58.23 %\n",
      "Epoch 5 of 100 took 2.483s\n",
      "  training loss (in-iteration): \t0.959770\n",
      "  validation accuracy: \t\t\t55.34 %\n",
      "Epoch 6 of 100 took 2.477s\n",
      "  training loss (in-iteration): \t0.906761\n",
      "  validation accuracy: \t\t\t62.81 %\n",
      "Epoch 7 of 100 took 2.474s\n",
      "  training loss (in-iteration): \t0.858963\n",
      "  validation accuracy: \t\t\t64.76 %\n",
      "Epoch 8 of 100 took 2.503s\n",
      "  training loss (in-iteration): \t0.823539\n",
      "  validation accuracy: \t\t\t64.75 %\n",
      "Epoch 9 of 100 took 2.455s\n",
      "  training loss (in-iteration): \t0.784336\n",
      "  validation accuracy: \t\t\t60.79 %\n",
      "Epoch 10 of 100 took 2.482s\n",
      "  training loss (in-iteration): \t0.753132\n",
      "  validation accuracy: \t\t\t65.95 %\n",
      "Epoch 11 of 100 took 2.499s\n",
      "  training loss (in-iteration): \t0.732197\n",
      "  validation accuracy: \t\t\t68.32 %\n",
      "Epoch 12 of 100 took 2.451s\n",
      "  training loss (in-iteration): \t0.707647\n",
      "  validation accuracy: \t\t\t61.90 %\n",
      "Epoch 13 of 100 took 2.494s\n",
      "  training loss (in-iteration): \t0.680916\n",
      "  validation accuracy: \t\t\t57.79 %\n",
      "Epoch 14 of 100 took 2.527s\n",
      "  training loss (in-iteration): \t0.659638\n",
      "  validation accuracy: \t\t\t71.48 %\n",
      "Epoch 15 of 100 took 2.552s\n",
      "  training loss (in-iteration): \t0.638346\n",
      "  validation accuracy: \t\t\t68.21 %\n",
      "Epoch 16 of 100 took 2.498s\n",
      "  training loss (in-iteration): \t0.620408\n",
      "  validation accuracy: \t\t\t70.28 %\n",
      "Epoch 17 of 100 took 2.528s\n",
      "  training loss (in-iteration): \t0.605668\n",
      "  validation accuracy: \t\t\t69.22 %\n",
      "Epoch 18 of 100 took 2.514s\n",
      "  training loss (in-iteration): \t0.581794\n",
      "  validation accuracy: \t\t\t71.33 %\n",
      "Epoch 19 of 100 took 2.538s\n",
      "  training loss (in-iteration): \t0.567055\n",
      "  validation accuracy: \t\t\t71.39 %\n",
      "Epoch 20 of 100 took 2.539s\n",
      "  training loss (in-iteration): \t0.554308\n",
      "  validation accuracy: \t\t\t72.02 %\n",
      "Epoch 21 of 100 took 2.520s\n",
      "  training loss (in-iteration): \t0.534996\n",
      "  validation accuracy: \t\t\t70.23 %\n",
      "Epoch 22 of 100 took 2.531s\n",
      "  training loss (in-iteration): \t0.519978\n",
      "  validation accuracy: \t\t\t72.14 %\n",
      "Epoch 23 of 100 took 2.543s\n",
      "  training loss (in-iteration): \t0.509710\n",
      "  validation accuracy: \t\t\t70.44 %\n",
      "Epoch 24 of 100 took 2.497s\n",
      "  training loss (in-iteration): \t0.495627\n",
      "  validation accuracy: \t\t\t68.64 %\n",
      "Epoch 25 of 100 took 2.530s\n",
      "  training loss (in-iteration): \t0.487411\n",
      "  validation accuracy: \t\t\t71.04 %\n",
      "Epoch 26 of 100 took 2.515s\n",
      "  training loss (in-iteration): \t0.471124\n",
      "  validation accuracy: \t\t\t71.36 %\n",
      "Epoch 27 of 100 took 2.529s\n",
      "  training loss (in-iteration): \t0.455336\n",
      "  validation accuracy: \t\t\t71.97 %\n",
      "Epoch 28 of 100 took 2.529s\n",
      "  training loss (in-iteration): \t0.447964\n",
      "  validation accuracy: \t\t\t72.22 %\n",
      "Epoch 29 of 100 took 2.506s\n",
      "  training loss (in-iteration): \t0.429232\n",
      "  validation accuracy: \t\t\t72.63 %\n",
      "Epoch 30 of 100 took 2.492s\n",
      "  training loss (in-iteration): \t0.423522\n",
      "  validation accuracy: \t\t\t70.41 %\n",
      "Epoch 31 of 100 took 2.535s\n",
      "  training loss (in-iteration): \t0.411312\n",
      "  validation accuracy: \t\t\t68.91 %\n",
      "Epoch 32 of 100 took 2.524s\n",
      "  training loss (in-iteration): \t0.401268\n",
      "  validation accuracy: \t\t\t71.92 %\n",
      "Epoch 33 of 100 took 2.483s\n",
      "  training loss (in-iteration): \t0.389116\n",
      "  validation accuracy: \t\t\t71.01 %\n",
      "Epoch 34 of 100 took 2.487s\n",
      "  training loss (in-iteration): \t0.382812\n",
      "  validation accuracy: \t\t\t71.64 %\n",
      "Epoch 35 of 100 took 2.438s\n",
      "  training loss (in-iteration): \t0.369529\n",
      "  validation accuracy: \t\t\t72.25 %\n",
      "Epoch 36 of 100 took 2.507s\n",
      "  training loss (in-iteration): \t0.361361\n",
      "  validation accuracy: \t\t\t72.64 %\n",
      "Epoch 37 of 100 took 2.491s\n",
      "  training loss (in-iteration): \t0.352959\n",
      "  validation accuracy: \t\t\t73.43 %\n",
      "Epoch 38 of 100 took 2.525s\n",
      "  training loss (in-iteration): \t0.342699\n",
      "  validation accuracy: \t\t\t73.21 %\n",
      "Epoch 39 of 100 took 2.498s\n",
      "  training loss (in-iteration): \t0.332145\n",
      "  validation accuracy: \t\t\t71.88 %\n",
      "Epoch 40 of 100 took 2.464s\n",
      "  training loss (in-iteration): \t0.324602\n",
      "  validation accuracy: \t\t\t73.59 %\n",
      "Epoch 41 of 100 took 2.471s\n",
      "  training loss (in-iteration): \t0.318526\n",
      "  validation accuracy: \t\t\t73.08 %\n",
      "Epoch 42 of 100 took 2.508s\n",
      "  training loss (in-iteration): \t0.311714\n",
      "  validation accuracy: \t\t\t72.36 %\n",
      "Epoch 43 of 100 took 2.505s\n",
      "  training loss (in-iteration): \t0.302101\n",
      "  validation accuracy: \t\t\t70.69 %\n",
      "Epoch 44 of 100 took 2.873s\n",
      "  training loss (in-iteration): \t0.295020\n",
      "  validation accuracy: \t\t\t73.66 %\n",
      "Epoch 45 of 100 took 2.644s\n",
      "  training loss (in-iteration): \t0.283567\n",
      "  validation accuracy: \t\t\t70.68 %\n",
      "Epoch 46 of 100 took 2.502s\n",
      "  training loss (in-iteration): \t0.280454\n",
      "  validation accuracy: \t\t\t69.28 %\n",
      "Epoch 47 of 100 took 2.545s\n",
      "  training loss (in-iteration): \t0.272979\n",
      "  validation accuracy: \t\t\t72.68 %\n",
      "Epoch 48 of 100 took 2.549s\n",
      "  training loss (in-iteration): \t0.265433\n",
      "  validation accuracy: \t\t\t73.10 %\n",
      "Epoch 49 of 100 took 2.465s\n",
      "  training loss (in-iteration): \t0.255202\n",
      "  validation accuracy: \t\t\t71.86 %\n",
      "Epoch 50 of 100 took 2.508s\n",
      "  training loss (in-iteration): \t0.253497\n",
      "  validation accuracy: \t\t\t73.04 %\n",
      "Epoch 51 of 100 took 3.040s\n",
      "  training loss (in-iteration): \t0.246298\n",
      "  validation accuracy: \t\t\t72.11 %\n",
      "Epoch 52 of 100 took 2.501s\n",
      "  training loss (in-iteration): \t0.240975\n",
      "  validation accuracy: \t\t\t71.63 %\n",
      "Epoch 53 of 100 took 2.498s\n",
      "  training loss (in-iteration): \t0.236767\n",
      "  validation accuracy: \t\t\t71.86 %\n",
      "Epoch 54 of 100 took 2.512s\n",
      "  training loss (in-iteration): \t0.231401\n",
      "  validation accuracy: \t\t\t73.22 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "batch_size = 50  # number of samples processed in one SGD iteration\n",
    "val_acc_mas = []\n",
    "N = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        logits = model(torch.as_tensor(\n",
    "            X_batch, dtype=torch.float32, device=device))\n",
    "        y_pred = logits.max(1)[1].cpu().data.numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))\n",
    "\n",
    "    # Early stopping\n",
    "    val_acc_mas.append(np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100)\n",
    "    if epoch >= N and val_acc_mas[epoch - N] == max(val_acc_mas[epoch - N:]):\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np0x_v0sWvaZ",
    "outputId": "6452e01a-4643-4c79-9156-da0167c956ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t73.33 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
    "    y_pred = logits.max(1)[1].cpu().data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
